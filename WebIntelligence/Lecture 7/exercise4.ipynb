{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/tdn/Downloads/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mgensim\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# Load the binary model (this takes a bit of time). The data set can be downloaded from, e.g., https://code.google.com/archive/p/word2vec/\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mKeyedVectors\u001b[39m.\u001b[39;49mload_word2vec_format(\u001b[39m'\u001b[39;49m\u001b[39m/Users/tdn/Downloads/GoogleNews-vectors-negative300.bin.gz\u001b[39;49m\u001b[39m'\u001b[39;49m, binary \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:1719\u001b[0m, in \u001b[0;36mKeyedVectors.load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[1;32m   1673\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_word2vec_format\u001b[39m(\n\u001b[1;32m   1674\u001b[0m         \u001b[39mcls\u001b[39m, fname, fvocab\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, binary\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mutf8\u001b[39m\u001b[39m'\u001b[39m, unicode_errors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mstrict\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   1675\u001b[0m         limit\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, datatype\u001b[39m=\u001b[39mREAL, no_header\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m   1676\u001b[0m     ):\n\u001b[1;32m   1677\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load KeyedVectors from a file produced by the original C word2vec-tool format.\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m \n\u001b[1;32m   1679\u001b[0m \u001b[39m    Warnings\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1717\u001b[0m \n\u001b[1;32m   1718\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1719\u001b[0m     \u001b[39mreturn\u001b[39;00m _load_word2vec_format(\n\u001b[1;32m   1720\u001b[0m         \u001b[39mcls\u001b[39;49m, fname, fvocab\u001b[39m=\u001b[39;49mfvocab, binary\u001b[39m=\u001b[39;49mbinary, encoding\u001b[39m=\u001b[39;49mencoding, unicode_errors\u001b[39m=\u001b[39;49municode_errors,\n\u001b[1;32m   1721\u001b[0m         limit\u001b[39m=\u001b[39;49mlimit, datatype\u001b[39m=\u001b[39;49mdatatype, no_header\u001b[39m=\u001b[39;49mno_header,\n\u001b[1;32m   1722\u001b[0m     )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/gensim/models/keyedvectors.py:2048\u001b[0m, in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype, no_header, binary_chunk_size)\u001b[0m\n\u001b[1;32m   2045\u001b[0m             counts[word] \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(count)\n\u001b[1;32m   2047\u001b[0m logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mloading projection weights from \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, fname)\n\u001b[0;32m-> 2048\u001b[0m \u001b[39mwith\u001b[39;00m utils\u001b[39m.\u001b[39;49mopen(fname, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m fin:\n\u001b[1;32m   2049\u001b[0m     \u001b[39mif\u001b[39;00m no_header:\n\u001b[1;32m   2050\u001b[0m         \u001b[39m# deduce both vocab_size & vector_size from 1st pass over file\u001b[39;00m\n\u001b[1;32m   2051\u001b[0m         \u001b[39mif\u001b[39;00m binary:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/smart_open/smart_open_lib.py:224\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, compression, transport_params)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m ve:\n\u001b[1;32m    222\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(ve\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 224\u001b[0m binary \u001b[39m=\u001b[39m _open_binary_stream(uri, binary_mode, transport_params)\n\u001b[1;32m    225\u001b[0m decompressed \u001b[39m=\u001b[39m so_compression\u001b[39m.\u001b[39mcompression_wrapper(binary, binary_mode, compression)\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mb\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mor\u001b[39;00m explicit_encoding \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/smart_open/smart_open_lib.py:400\u001b[0m, in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    398\u001b[0m scheme \u001b[39m=\u001b[39m _sniff_scheme(uri)\n\u001b[1;32m    399\u001b[0m submodule \u001b[39m=\u001b[39m transport\u001b[39m.\u001b[39mget_transport(scheme)\n\u001b[0;32m--> 400\u001b[0m fobj \u001b[39m=\u001b[39m submodule\u001b[39m.\u001b[39;49mopen_uri(uri, mode, transport_params)\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(fobj, \u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    402\u001b[0m     fobj\u001b[39m.\u001b[39mname \u001b[39m=\u001b[39m uri\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/smart_open/local_file.py:34\u001b[0m, in \u001b[0;36mopen_uri\u001b[0;34m(uri_as_string, mode, transport_params)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mopen_uri\u001b[39m(uri_as_string, mode, transport_params):\n\u001b[1;32m     33\u001b[0m     parsed_uri \u001b[39m=\u001b[39m parse_uri(uri_as_string)\n\u001b[0;32m---> 34\u001b[0m     fobj \u001b[39m=\u001b[39m io\u001b[39m.\u001b[39;49mopen(parsed_uri[\u001b[39m'\u001b[39;49m\u001b[39muri_path\u001b[39;49m\u001b[39m'\u001b[39;49m], mode)\n\u001b[1;32m     35\u001b[0m     \u001b[39mreturn\u001b[39;00m fobj\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/tdn/Downloads/GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# Load the binary model (this takes a bit of time). The data set can be downloaded from, e.g., https://code.google.com/archive/p/word2vec/\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/tdn/Downloads/GoogleNews-vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired by http://web.stanford.edu/class/cs224n/\n",
    "def display_pca_scatterplot(model, words=None, sample=0, save=False, file_name='scatterplot'):\n",
    "    if words == None:\n",
    "        if sample > 0:\n",
    "            words = np.random.choice(list(model.key_to_index.keys()), sample)\n",
    "        else:\n",
    "            words = [ word for word in model.key_to_index.keys() ]\n",
    "        \n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    twodim = PCA(n_components=2).fit_transform(word_vectors)[:,:2]\n",
    "    \n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "    plt.xlabel('Dim 1', fontsize=10)\n",
    "    plt.ylabel('Dim 2', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    if save:\n",
    "        plt.savefig(file_name + '.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.most_similar('obama')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
