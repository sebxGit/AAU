{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dacb06e",
   "metadata": {},
   "source": [
    "# Self study 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-attempt",
   "metadata": {},
   "source": [
    "## Libraries\n",
    "For this task you will need the PyKEEN library, which implements several knowledge graph embedding methods. You can find its documentation [here](https://pykeen.readthedocs.io)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-width",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykeen.pipeline import pipeline, plot_losses, plot_er\n",
    "from pykeen.models import TransE, DistMult, RESCAL\n",
    "from pykeen.datasets import Nations, get_dataset\n",
    "from pykeen.predict import predict_triples, predict_target\n",
    "from pykeen.evaluation import RankBasedEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-chess",
   "metadata": {},
   "source": [
    "## Embeddings computation\n",
    "In this session, we will learn how to compute knowledge graph embeddings using the PyKEEN library.\n",
    "\n",
    "This self-study module is based on the [PyKEEN tutorial](https://pykeen.readthedocs.io/en/latest/tutorial/first_steps.html). We will use TransE to generate the embeddings, but other methods are possible, such as DistMult and RESCAL, which we studied during the lecture. They are already imported, it's enough to change the next code snippet to use a different method.\n",
    "\n",
    "As the dataset, we will use the Nations dataset. This knowledge graph is very small: it contains 14 nodes, 55 predicates and 1992 triples. However, it allows to execute PyKEEN also on PC not equipped for heavy machine learning training, with GPU and so on. If you have a machine setup to execute PyTorch using GPU, you can change Nations with another dataset - such as [FB15k237](https://pykeen.readthedocs.io/en/stable/api/pykeen.datasets.FB15k237.html).\n",
    "\n",
    "The easiest way to use PyKEEN is through the pipeline function. This function allows to declare the information about the KGE method and the dataset, and it trains the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64d3f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pipeline(\n",
    "    dataset=Nations,\n",
    "    model=TransE,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fc48fd",
   "metadata": {},
   "source": [
    "The pipeline function can be extended with several parameters, which affect the training of the model. For example, one can set the learning rate, or change the negative sampling algorithm. The [documentation](https://pykeen.readthedocs.io/en/latest/api/pykeen.pipeline.pipeline.html#pykeen.pipeline.pipeline) of the function illustrates all the options.\n",
    "\n",
    "## Visualisation\n",
    "We can now investigate the results of the training process. First, we can use the plot_losses function to visualise how the loss function changes over the different epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b8e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f240c5b9",
   "metadata": {},
   "source": [
    "We can also visualise the embeddings. The plot_er function visualise the vectors in a 2D space. The following plot shows the embeddings of the entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0011d9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_er(result, plot_relations=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be247149",
   "metadata": {},
   "source": [
    "## Triple scores\n",
    "We can now use the model for prediction tasks. First, we use the predict_triples function to predict the triple score of a set of statements. In the following, we use the validation dataset provided by PyKEEN for the Nations dataset. \n",
    "\n",
    "The resulting object, pack, contains pairs of statements and their predicted score. The pack object is enriched with the node and edge labels, and is exported in a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset=\"nations\")\n",
    "pack = predict_triples(model=result.model, triples=dataset.validation)\n",
    "df = pack.process(factory=result.training).df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3cfec1",
   "metadata": {},
   "source": [
    "This command retrieves the five statements that obtained the highest scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29036cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.nlargest(n=5, columns=\"score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d012c5b",
   "metadata": {},
   "source": [
    "We can retrieve the nodes and the average score of the statements where they are involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2085af59",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"head_id\", \"head_label\"]).agg({\"score\": [\"mean\", \"std\", \"count\"]}).sort_values(by=(\"score\", \"mean\"), ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee638ae",
   "metadata": {},
   "source": [
    "## Link prediction\n",
    "Instead of computing the triple scores, one can perform a link prediction task, predicting the object (a.k.a. tail) given the subject (a.k.a. head) and the predicate (a.k.a. relation). The function to perform link prediction is predict_target. This code executes the query (uk, conferences, ?). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f261c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_target(\n",
    "    model=result.model,\n",
    "    head=\"uk\",\n",
    "    relation=\"conferences\",\n",
    "    triples_factory=result.training,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd12eeef",
   "metadata": {},
   "source": [
    "To process the predictions, we first perform a filter step, i.e., we remove the statements which were already included in the training set. We used them to learn, so there is very little value in predicting something that we know to be true from the beginning. \n",
    "\n",
    "Next, we enrich the predictions with information about the presence of the statements in the validation or testing datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf315578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_filtered = pred.filter_triples(dataset.training)\n",
    "pred_annotated = pred_filtered.add_membership_columns(validation=dataset.validation, testing=dataset.testing)\n",
    "pred_annotated.df.sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef2804b",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Evaluation is an essential step to assess the quality of a model, as well to compare different KGE methods. A typical way to evaluate KGE methods is to use rank based evaluation, treating the KGE model as a recommender system. Given a query, a model can return a sequence of potential answers, ordered by the triple score associated to the relative statement. Therefore, given a test set that includes positive statements, one can submit link prediction queries to the model, and study how the expected answer is ranked in the results. \n",
    "\n",
    "The following code evaluates the model. The evaluate method takes as inputs the trained model, the test set, and the list statements used during training and validation, so that they can be filtered out from the query answers.\n",
    "\n",
    "There are typical three metrics that are studied when evaluating KGE methods: the mean rank (MR), the mean reciprocal rank (MRR), and Hits@k. Their definition are available e.g. in [Wikipedia](https://en.wikipedia.org/wiki/Knowledge_graph_embedding#Performance_indicators)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d7250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RankBasedEvaluator()\n",
    "metrics = evaluator.evaluate(result.model, dataset.testing.mapped_triples, additional_filter_triples=[dataset.training.mapped_triples, dataset.validation.mapped_triples])\n",
    "print(f\"Mean rank: {metrics.get_metric('mean_rank'):.2f}\")\n",
    "print(f\"Mean reciprocal rank: {metrics.get_metric('mean_reciprocal_rank'):.2f}\")\n",
    "print(f\"Hits@10: {metrics.get_metric('hits@10'):.2%}\")\n",
    "print(f\"Hits@1: {metrics.get_metric('hits@1'):.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
