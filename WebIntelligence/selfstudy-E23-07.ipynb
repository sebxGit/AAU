{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9397154-7874-4413-b371-3affe83b31f5",
   "metadata": {},
   "source": [
    "# Self study 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf7a374-4210-43e5-9fe2-82c0f3f94aea",
   "metadata": {},
   "source": [
    "In this self study we will experiment with text classification using word embeddings. Specifically, we will use the prelearned embeddings that was also used during the lecture and which can be downloaded from https://code.google.com/archive/p/word2vec/. There are two aspects to the self study: 1. to play around with word embeddings and 2. to get some experience using these embedding for text modeling tasks such as text classification. The latter also entails working with relevant models, which in this self study are in the form of neural network models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293c936b-cf85-45ef-a331-4e84d7816dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "import gensim\n",
    "import certifi\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8172b-2cc9-434e-a9d4-b0d63f95a3aa",
   "metadata": {},
   "source": [
    "## Getting the embedding model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6a5613-fae7-4b42-9a9b-0a4c7e2cd7fb",
   "metadata": {},
   "source": [
    "We again use the prelearned embeddings from the lecture. Note, loading these embeddings can be a bit time consuming so a bit of patience is advised ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "234b175c-e218-48d7-bf22-8cbb16e0ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Users/sebastiantruong/Downloads/GoogleNews-vectors-negative300.bin.gz', binary = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f197d657-27ef-4ae7-bd1a-fe89c28f7ebc",
   "metadata": {},
   "source": [
    "Extract a few example words from the embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aff24be-3058-43f6-ad46-2e4ddd27826e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['</s>', 'in', 'for', 'that', 'is', 'on', '##', 'The', 'with', 'said']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.key_to_index.keys())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2985a0b1",
   "metadata": {},
   "source": [
    "We can also get the embdding associated with a particular word (here only the first 10 entries of one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46821822-379a-40a1-80cb-5cfef3da267c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.29492188, -0.32617188, -0.39453125, -0.08691406, -0.03833008,\n",
       "        0.07568359, -0.07958984, -0.06640625,  0.39257812, -0.1953125 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['IMDB'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f81eee4-a5cb-4042-8b3e-8203aa50ec7e",
   "metadata": {},
   "source": [
    "## Loading and processing the IMDB data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1b6d79-2fde-4052-b53b-879dca86768d",
   "metadata": {},
   "source": [
    "We use a data set from the Internet Movie database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41c5e1a6-3493-476f-862e-9fad58b425f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = load_dataset(\"imdb\")\n",
    "reviews_train = imdb['train'][\"text\"]\n",
    "reviews_test = imdb['test'][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a910eff-ce93-4314-af0a-fc2a5089c914",
   "metadata": {},
   "source": [
    "Example review and label ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15ac04f3-faaf-4998-b6fb-7cc0ebcb635d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: WARNING: This review contains SPOILERS. Do not read if you don't want some points revealed to you before you watch the film.<br /><br />With a cast like this, you wonder whether or not the actors and actresses knew exactly what they were getting into. Did they see the script and say, `Hey, Close Encounters of the Third Kind was such a hit that this one can't fail.' Unfortunately, it does. Did they even think to check on the director's credentials? I mean, would YOU do a movie with the director of a movie called `Satan's Cheerleaders?' Greydon Clark, who would later go on to direct the infamous `Final Justice,' made this. It makes you wonder how the people of Mystery Science Theater 3000 could hammer `Final Justice' and completely miss out on `The Return.'<br /><br />The film is set in a small town in New Mexico. A little boy and girl are in the street unsupervised one night when a powerful flashlight beam.er.a spaceship appears and hovers over them. In probably the worst special effect sequence of the film, the ship spews some kind of red ink on them. It looked like Clark had held a beaker of water in from of the camera lens and dipped his leaky pen in it, so right away you are treated with cheese. Anyhow, the ship leaves and the adults don't believe the children. Elsewhere, we see Vincent Schiavelli, whom I find to be a terrific actor (watch his scenes in `Ghost' for proof, as they are outstanding), who is playing a prospector, or as I called him, the Miner 1949er. He steps out of the cave he is in, and he and his dog are inked by the ship. Twenty-five years go by, and the girl has grown up to be Cybill Shepherd, who works with her father, Raymond Burr, in studying unusual weather phenomena. Or something like that. Shepherd spots some strange phenomena in satellite pictures over that little New Mexico town, and she travels there to research it. Once she gets there, the local ranchers harass her, and blame her for the recent slew of cattle mutilations that have been going on, and deputy Jan-Michael Vincent comes to her rescue. From this point on, the film really drags as the two quickly fall for each other, especially after Vincent wards off the locals and informs Shepherd that he was the little boy that saw the ship with her twenty-five years earlier. While this boring mess is happening, Vincent Schiavelli, with his killer dog at his side, is walking around killing the cattle and any people he runs into with an unusual item. You know those glowing plastic sticks stores sell for trick-or-treaters at Halloween, the kind that you shake to make them glow? Schiavelli uses what looks like one of those glow sticks to burn incisions in people. It's the second-worst effect in the movie. Every time Schiavelli is on screen with the glow stick, the scene's atmosphere suddenly turns dark, like the filmmakers thought the glow stick needed that enhancement. It ends up making the movie look even cheaper than it is.<br /><br />And what does all this lead up to? It's hard to tell when the final, confusing scene arrives. See, Burr and his team of scientists try to explain the satellite images that Shepherd found as some kind of `calling card,' but none of it makes sense. Why do Shepherd and Vincent age and Schiavelli does not? Schiavelli explains why he is killing cattle and people and why he wants Shepherd dead, but even that doesn't make much sense when you really think about it. I mean, why doesn't he kill Jan-Michael Vincent? After all, he had twenty-five years to do it. And the aliens won't need him if Shepherd is dead anyhow, so why try to kill her? Speaking of the aliens, it is never clear what they really wanted out of Shepherd and Vincent. What is their goal? Why do they wait so long to intervene? How could they be so sure Shepherd would come back? Not that the answer to any of these and other questions would have made `The Return' any more pleasant. You would still have bad lines, really bad acting, particularly by Shepherd, cheesy effects, and poor direction. Luckily, the stars escaped from this movie. Cybill Shepherd soon went on to star in `Moonlighting' with Bruce Willis. Jan-Michael Vincent went on to be featured in dozens of B-movies, often in over-the-top parts. Raymond Burr made a pile of Perry Mason television movies right up until his death. Vincent Schiavelli went on to be a great character actor in a huge number of films. Martin Landau, who played a kooky law enforcement officer, quickly made the terrific `Alone in the Dark' and the awful `The Being' before rolling into the films he has been famous for recently. You can bet none of these stars ever want their careers to return to `The Return.' Zantara's score: 2 out of 10.\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "review_idx = 42\n",
    "print(\"Review: \" + imdb['train']['text'][review_idx])\n",
    "print(\"Label: \" + str(imdb['train']['label'][review_idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc3385-8b25-4522-9e41-c4bd0cc6e83d",
   "metadata": {},
   "source": [
    "We construct a dictionary for the reviews in reviews_train,  but only include terms with a minimum frequency of 0.0005 and a maximum frequency of 0.5. More specialized types of text preprocessing could also be performed, but that is less important for this self study and is therefore left out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d4e2bb-6d4f-43d1-a391-d6b24fb4ce85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary=CountVectorizer(min_df=0.0005, max_df=0.5).fit(reviews_train)\n",
    "dictionary=CountVectorizer(min_df=0.001, max_df=0.5).fit(reviews_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb51329f-483d-4fbd-897b-d586b3b77951",
   "metadata": {},
   "source": [
    "Based on the dictionary we can get matrix/array representations of the data sets containing the counts of the invdividual words in the reviews. Thus, for each data set we have a matrix of size (#reviews x #words_in_dictionary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f0178ce-56cc-4bff-909b-bab779aecfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of reviews_train:(25000, 10430)\n"
     ]
    }
   ],
   "source": [
    "# Sparse matrix representation\n",
    "reviews_train_tf=dictionary.transform(reviews_train)\n",
    "reviews_test_tf=dictionary.transform(reviews_test)\n",
    "\n",
    "# reviews_train_tf is a numpy matrix, so we convert it to an array. Same things applies to the test data\n",
    "reviews_train_tf = np.squeeze(np.asarray(reviews_train_tf.todense()))\n",
    "reviews_test_tf = np.squeeze(np.asarray(reviews_test_tf.todense()))\n",
    "print(f\"Shape of reviews_train:{reviews_train_tf.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00942734-f0bd-4bc7-99ac-8844a0e28008",
   "metadata": {},
   "source": [
    "Next we construct embedding matrices for reviews_train and reviews_test. An embedding matrix has size #words_in_dictionary x #embedding_size. The matrix is constructed by iterating over the words in the dictionary we constructed above. If a word does not have an embedding (i.e., it is not contained in the embedding model), it is effectively given an embedding consisting of zeros only. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "868e595b-34ba-45ac-84c6-d6c0290ef2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of failed look up attempts for the dictionary: 502\n",
      "Examples include: ['1967', '40', 'bergman', '17', 'godard', '10', 'lucille', 'ritter', 'bogdanovich', 'audrey']\n"
     ]
    }
   ],
   "source": [
    "train_emb_matrix = np.zeros((reviews_train_tf.shape[1], 300))\n",
    "words_not_found = []\n",
    "for k, i in dictionary.vocabulary_.items():\n",
    "    try: \n",
    "        train_emb_matrix[i,:] = model[k]\n",
    "    except KeyError:\n",
    "        words_not_found.append(k)\n",
    "print(f\"Number of failed look up attempts for the dictionary: {len(words_not_found)}\")\n",
    "print(f\"Examples include: {words_not_found[:10]}\")\n",
    "        \n",
    "test_emb_matrix = np.zeros((reviews_test_tf.shape[1], 300))\n",
    "for k, i in dictionary.vocabulary_.items():\n",
    "    try: \n",
    "        test_emb_matrix[i,:] = model[k]\n",
    "    except KeyError:\n",
    "        None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa83c8e-5e88-4309-a67e-55aab376553d",
   "metadata": {},
   "source": [
    "Next we represent each movie review by averaging over the embeddings of the words in the review. This is achieved by first finding the normalized word frequencies and afterwards combining them with the embedding matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2182f074-61d5-4ddf-a889-425aa884ef3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The resulting matrix (review_train_word2vec) shape: (25000, 300)\n"
     ]
    }
   ],
   "source": [
    "reviews_train_tf_norm =  reviews_train_tf / np.sum(reviews_train_tf, axis=1, keepdims=True)\n",
    "reviews_test_tf_norm =  reviews_test_tf / np.sum(reviews_test_tf, axis=1, keepdims=True)\n",
    "review_train_word2vec = reviews_train_tf_norm @ train_emb_matrix\n",
    "review_test_word2vec = reviews_test_tf_norm @ test_emb_matrix\n",
    "print(f\"The resulting matrix (review_train_word2vec) shape: {review_train_word2vec.shape}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c901dc-49b9-463e-860d-0eb584f3bb6d",
   "metadata": {},
   "source": [
    "## Task\n",
    "\n",
    "Below is a small example of how to set up a simple neural network using PyTorch to experiment with classification of the movie reviews using their embedding representations. In this task you should:\n",
    " * Experiment with different neural network archietctures and learning settings to investigate the effects on the classification accuracy. To support this analysis, revisit the data processing setup above and define a validation data set that can be used during model learning. \n",
    " * Try changing the cutoff frequencies when constructing the IMDB dictionary. How does changing these parameters affect that accuracy results of your model?\n",
    "\n",
    "In the example code below, we work with the torch.nn module provided by PyTorch. A short introduction to this module and how to define neural networks in PyTorch can be found at\n",
    "\n",
    " https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py\n",
    " https://pytorch.org/tutorials/beginner/nn_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db4b4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2f743ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###Experiment with different NN architecture and learning settings and how it affects the classification accuracy\n",
    "#tried two different NN layers setups, but with a similar accuracy of approx. 86%\n",
    "#changing adam learning rate #lr=0.01 83%acc, #lr=0.0001 86.1%acc, \n",
    "\n",
    "###Try changing the cutoff frequencies\n",
    "#The chosen standard value works well, and an increase in the min_df seems to reduce\n",
    "#the accuracy by some percentages, as some reviews are filtered off.\n",
    "#tested: (min_df=0.0005, max_df=0.5) Test Error: Accuracy: 86.1%, Avg loss: 0.326928 \n",
    "#tested: (min_df=0.0005, max_df=0.7) Test Error: Accuracy: 85.6%, Avg loss: 0.336738\n",
    "#tested: (min_df=0.0005, max_df=0.9) Test Error: Accuracy: 85.8%, Avg loss: 0.335870\n",
    "#tested: (min_df=0.001 , max_df=0.5) Test Error: Accuracy: 85.6%, Avg loss: 0.337421 \n",
    "#tested: (min_df=0.02  , max_df=0.5) Test Error: Accuracy: 83.5%, Avg loss: 0.374771 \n",
    "#tested: (min_df=0.02  , max_df=0.9) Test Error: Accuracy: 83.6%, Avg loss: 0.377356 \n",
    "\n",
    "#Changing batch size had an effect\n",
    "# batch size 8  did:  Test Error: Accuracy: 86.0%, Avg loss: 0.329796\n",
    "# batch size 32 did:  Test Error: Accuracy: 85.7%, Avg loss: 0.332952 \n",
    "# batch size 64 did:  Test Error: Accuracy: 86.1%, Avg loss: 0.326928 \n",
    "# batch size 128 did: Test Error: Accuracy: 85.5%, Avg loss: 0.340475\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d76ac6-bec6-4358-897b-8aba45ef0dd9",
   "metadata": {},
   "source": [
    "### Simple neural network implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9759f533-a56f-46b5-ad1b-34ac82d34617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import Adam\n",
    "import evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4940959a-1cb9-45fd-a5b7-cc18df9690f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90da6ad7-666f-4f2b-aa6c-619cbcd58a6a",
   "metadata": {},
   "source": [
    "Define a dataloader for manageing the data set. Here we still be working with minibatches of size 64, which forms the basis for a stochastic gradient descent implementation for learning the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d285020-67aa-4507-9486-6510dda02f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(torch.tensor(review_train_word2vec, dtype=torch.float), torch.tensor(imdb['train']['label']))\n",
    "test = torch.utils.data.TensorDataset(torch.tensor(review_test_word2vec, dtype=torch.float), torch.tensor(imdb['test']['label']))\n",
    "batch_size = 8\n",
    "train_loader = torch.utils.data.DataLoader(train, \n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True,\n",
    "                                           num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(test, \n",
    "                                           batch_size=1,\n",
    "                                           shuffle=False,\n",
    "                                           drop_last=False,\n",
    "                                           num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908411c-7c64-47d2-b736-09522bbf53f4",
   "metadata": {},
   "source": [
    "Define the neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "295f6367-9d8c-40af-96bf-a133998824e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard\n",
    "#Test Error: \n",
    "#Accuracy: 86.1%, Avg loss: 0.326928 \n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(300, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):        \n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "#test1\n",
    "#Test Error: \n",
    "#Accuracy: 86.0%, Avg loss: 0.328519\n",
    "# class NeuralNetwork(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.flatten = nn.Flatten()\n",
    "#         self.linear_relu_stack = nn.Sequential(\n",
    "#             nn.Linear(300, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 512),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(512, 2),\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.flatten(x)\n",
    "#         logits = self.linear_relu_stack(x)\n",
    "#         return logits\n",
    "\n",
    "#test2\n",
    "#Test Error: \n",
    "#Accuracy: 86.2%, Avg loss: 0.324331     \n",
    "# class NeuralNetwork(nn.Module):\n",
    "# \tdef __init__(self):\n",
    "# \t\t\tsuper().__init__()\n",
    "# \t\t\tself.linear_relu_stack = nn.Sequential(\n",
    "# \t\t\t\t\tnn.Linear(300, 128),\n",
    "# \t\t\t\t\tnn.ReLU(),\n",
    "# \t\t\t\t\tnn.Linear(128, 128),\n",
    "# \t\t\t\t\tnn.Linear(128, 300),\n",
    "# \t\t\t\t\tnn.Sigmoid(),\n",
    "# \t\t\t\t\tnn.Linear(300, 2),\n",
    "# \t\t\t)\n",
    "\n",
    "# \tdef forward(self, x):        \n",
    "# \t\t\tlogits = self.linear_relu_stack(x)\n",
    "# \t\t\treturn logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "262739ca-bf91-4469-ab60-0e6cf48444cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=300, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=2, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9607145b-802e-462a-87e5-feac70cee350",
   "metadata": {},
   "source": [
    "Set up methods for the training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c0878c42-b6cb-47c0-8be0-b6c09ff1b86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://pytorch.org/tutorials/beginner/basics/optimization_tutorial.html\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        train_loss += loss.item()\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    return train_loss / len(dataloader)\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss, correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1033af65-7969-4c01-bc6e-f89a0847ea22",
   "metadata": {},
   "source": [
    "Lastly, let's take the model out for a spin and see how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6ebeee0-9f2e-4425-abd1-59f168ce4c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 50.0%, Avg loss: 0.693273 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.41, Test_loss: 0.35, accuracy: 85.02:  10%|█         | 1/10 [00:06<00:55,  6.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.0%, Avg loss: 0.350543 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.35, Test_loss: 0.35, accuracy: 84.70:  20%|██        | 2/10 [00:12<00:49,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 84.7%, Avg loss: 0.352131 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.34, Test_loss: 0.34, accuracy: 85.38:  30%|███       | 3/10 [00:18<00:44,  6.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.344042 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.34, Test_loss: 0.34, accuracy: 85.47:  40%|████      | 4/10 [00:24<00:37,  6.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.341767 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.34, Test_loss: 0.34, accuracy: 85.50:  50%|█████     | 5/10 [00:31<00:31,  6.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.5%, Avg loss: 0.338778 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.33, Test_loss: 0.34, accuracy: 85.38:  60%|██████    | 6/10 [00:37<00:24,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.4%, Avg loss: 0.339280 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.33, Test_loss: 0.33, accuracy: 85.81:  70%|███████   | 7/10 [00:43<00:18,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.8%, Avg loss: 0.333113 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.33, Test_loss: 0.34, accuracy: 85.34:  80%|████████  | 8/10 [00:50<00:12,  6.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.3%, Avg loss: 0.338140 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.33, Test_loss: 0.33, accuracy: 85.85:  90%|█████████ | 9/10 [00:56<00:06,  6.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 85.9%, Avg loss: 0.331768 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train_loss: 0.32, Test_loss: 0.33, accuracy: 85.97: 100%|██████████| 10/10 [01:03<00:00,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.329796 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 86.0%, Avg loss: 0.329796 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "epochs = 10\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5**(2/epochs))\n",
    "\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "\n",
    "pbar = tqdm(range(epochs))\n",
    "for epoch in pbar:\n",
    "    train_loss = train_loop(train_loader, model, loss_fn, optimizer)\n",
    "    test_loss, accuracy = test_loop(test_loader, model, loss_fn)\n",
    "    scheduler.step()\n",
    "    pbar.set_description(f\"Train_loss: {train_loss:.2f}, Test_loss: {test_loss:.2f}, accuracy: {100*accuracy:0.2f}\")\n",
    "\n",
    "test_loop(test_loader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b376c35-5340-41d1-bbfb-664c3cdd0ced",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
