{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 2\n",
    "\n",
    "\n",
    "In this self-study we build an index that supports Boolean search over the web pages that you crawl with the crawler from the 1st self study. You can continue to just extract the titles of the web-pages you crawl, or you can be more adventurous and look at the whole text that you get from the .get_text() method of a BeautifulSoup parser. In either case, the collection of texts from the crawled web-pages is you corpus. You should then:\n",
    "\n",
    "- construct the vocabulary of terms for your corpus\n",
    "- build an 'inverted' index for your vocabulary\n",
    "- implement Boolean search for your index (perhaps only for a limited set of Boolean queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some things already used in self study 1:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful resource is the nltk natural language processing package:\n",
    "https://www.nltk.org/\n",
    "which provides methods for tokenization, stemming, and much more (the 'punkt' package is needed for tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\minhs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the title string of the AAU homepage as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAU - Viden for verden - Aalborg Universitet\n"
     ]
    }
   ],
   "source": [
    "r=requests.get('https://www.aau.dk/')\n",
    "r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "string=r_parse.find('title').string\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAU\n",
      "-\n",
      "Viden\n",
      "for\n",
      "verden\n",
      "-\n",
      "Aalborg\n",
      "Universitet\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(string)\n",
    "for t in tokens:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can stem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aau\n",
      "-\n",
      "viden\n",
      "for\n",
      "verden\n",
      "-\n",
      "aalborg\n",
      "universitet\n"
     ]
    }
   ],
   "source": [
    "ps=nltk.PorterStemmer()\n",
    "for t in tokens:\n",
    "    print(ps.stem(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Danish language the Porter stemmer will not be terribly useful! There is also a Danish option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aau\n",
      "-\n",
      "vid\n",
      "for\n",
      "verd\n",
      "-\n",
      "aalborg\n",
      "universit\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    print(dstemmer.stem(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aau', '-', 'viden', 'for', 'verden', 'aalborg', 'universitet', 'universitetsuddannels', 'videregend', 'uddannels', 'p', 'kandidatuddannels', 'sidefag', 'og', 'tilvalgsfag', 'studieby', 'her', 'kan', 'du', 'studer', 'su', 'sp', 'stttemulighed', 'forskn', 'forskningsnyt', 'fra']\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "corpus = []\n",
    "postings = []\n",
    "\n",
    "numOfArticles = 10\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")\n",
    "\n",
    "def getTitles(link):\n",
    "    titles = []\n",
    "\n",
    "    rp=RobotFileParser()\n",
    "    rp.set_url(link)\n",
    "    rp.read()\n",
    "    r=requests.get(link)\n",
    "\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    r_parse.find('title').string\n",
    "    for i, a in enumerate(r_parse.find_all('a')):\n",
    "        if(i == numOfArticles): break\n",
    "        _link = a['href']\n",
    "        if(_link == '#main'):   continue\n",
    "        if(_link[0] == '/'):    _link = link+_link[1:]\n",
    "        titles.append(_getTitles(_link))\n",
    "    return titles\n",
    "\n",
    "def _getTitles(link):\n",
    "    r=requests.get(link)\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    return r_parse.find('title').string\n",
    "\n",
    "def remove_non_ascii(a_str):\n",
    "    ascii_chars = set(string.printable)\n",
    "\n",
    "    return ''.join(\n",
    "        filter(lambda x: x in ascii_chars, a_str)\n",
    "    )\n",
    "\n",
    "def tokenizeAndStemTitles(titles):\n",
    "    _postings = []\n",
    "    tokens = []\n",
    "    for i, title in enumerate(titles):\n",
    "        _tokens=nltk.word_tokenize(title)\n",
    "        ps=nltk.PorterStemmer()\n",
    "        for t in _tokens:\n",
    "            s = ps.stem(t)\n",
    "            s = remove_non_ascii(s)\n",
    "            if(s == ''): continue\n",
    "            if s not in tokens: tokens.append(s)\n",
    "\n",
    "            flag = 0\n",
    "            for el in _postings:\n",
    "                if el['vocabulary'] == s:\n",
    "                    el['postings'].append(i)\n",
    "                    flag = 1\n",
    "\n",
    "            if flag == 0: _postings.append(dict(vocabulary=s, postings=[i]))\n",
    "\n",
    "    return tokens, _postings\n",
    "\n",
    "titles = getTitles('https://www.aau.dk/')\n",
    "\n",
    "\n",
    "#construct the vocabulary of terms for your corpus (corpus)\n",
    "#build an 'inverted' index for your vocabulary (postings)\n",
    "corpus, postings = tokenizeAndStemTitles(titles)\n",
    "print(corpus)\n",
    "#implement Boolean search for your index (perhaps only for a limited set of Boolean queries)\n",
    "query = input(\"what is ur boolean query? \\n(AND, NOT, OR | aau AND viden)\")\n",
    "\n",
    "def booleanSearch(query):\n",
    "    pass\n",
    "\n",
    "booleanSearch(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\minhs\\Desktop\\selfstudy-E23-02.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/minhs/Desktop/selfstudy-E23-02.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m k \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mchoose k\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/minhs/Desktop/selfstudy-E23-02.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m pyth \u001b[39m=\u001b[39m [(x,y,z) \u001b[39mfor\u001b[39;00m x,y,z \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39;49m(k) \u001b[39mif\u001b[39;00m x\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39my\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m==\u001b[39mz\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/minhs/Desktop/selfstudy-E23-02.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(pyth)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "k = 10\n",
    "pyth = [(x,y,z) for x,y,z in range(k) if x**2+y**2==z**2]\n",
    "print(pyth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is most useful for you depends on which websites you crawl. It is not essential for the exercise that the stemming always is the best possible ...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
