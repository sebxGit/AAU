{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self study 2\n",
    "\n",
    "\n",
    "In this self-study we build an index that supports Boolean search over the web pages that you crawl with the crawler from the 1st self study. You can continue to just extract the titles of the web-pages you crawl, or you can be more adventurous and look at the whole text that you get from the .get_text() method of a BeautifulSoup parser. In either case, the collection of texts from the crawled web-pages is you corpus. You should then:\n",
    "\n",
    "- construct the vocabulary of terms for your corpus\n",
    "- build an 'inverted' index for your vocabulary\n",
    "- implement Boolean search for your index (perhaps only for a limited set of Boolean queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some things already used in self study 1:\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful resource is the nltk natural language processing package:\n",
    "https://www.nltk.org/\n",
    "which provides methods for tokenization, stemming, and much more (the 'punkt' package is needed for tokenization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sebas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use the title string of the AAU homepage as an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAU - Viden for verden - Aalborg Universitet\n"
     ]
    }
   ],
   "source": [
    "r=requests.get('https://www.aau.dk/')\n",
    "r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "string=r_parse.find('title').string\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAU\n",
      "-\n",
      "Viden\n",
      "for\n",
      "verden\n",
      "-\n",
      "Aalborg\n",
      "Universitet\n"
     ]
    }
   ],
   "source": [
    "tokens=nltk.word_tokenize(string)\n",
    "for t in tokens:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can stem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aau\n",
      "-\n",
      "viden\n",
      "for\n",
      "verden\n",
      "-\n",
      "aalborg\n",
      "universitet\n"
     ]
    }
   ],
   "source": [
    "ps=nltk.PorterStemmer()\n",
    "for t in tokens:\n",
    "    print(ps.stem(t))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Danish language the Porter stemmer will not be terribly useful! There is also a Danish option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aau\n",
      "-\n",
      "vid\n",
      "for\n",
      "verd\n",
      "-\n",
      "aalborg\n",
      "universit\n"
     ]
    }
   ],
   "source": [
    "for t in tokens:\n",
    "    print(dstemmer.stem(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: ['aau', 'viden', 'for', 'verden', 'aalborg', 'universitet', 'universitetsuddannels', 'videregend', 'uddannels', 'p', 'kandidatuddannels', 'sidefag', 'og', 'tilvalgsfag', 'studieby', 'her', 'kan', 'du', 'studer', 'su', 'sp', 'stttemulighed', 'forskn', 'forskningsnyt', 'fra', 'ph.d.uddannels']\n",
      "inverted matrix: [{'vocabulary': 'aau', 'postings': [0, 1, 4]}, {'vocabulary': 'viden', 'postings': [0]}, {'vocabulary': 'for', 'postings': [0]}, {'vocabulary': 'verden', 'postings': [0]}, {'vocabulary': 'aalborg', 'postings': [0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]}, {'vocabulary': 'universitet', 'postings': [0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]}, {'vocabulary': 'universitetsuddannels', 'postings': [1]}, {'vocabulary': 'videregend', 'postings': [1]}, {'vocabulary': 'uddannels', 'postings': [1]}, {'vocabulary': 'p', 'postings': [1, 2, 3, 4, 5, 6, 7, 9]}, {'vocabulary': 'kandidatuddannels', 'postings': [2]}, {'vocabulary': 'sidefag', 'postings': [3]}, {'vocabulary': 'og', 'postings': [3]}, {'vocabulary': 'tilvalgsfag', 'postings': [3]}, {'vocabulary': 'studieby', 'postings': [4]}, {'vocabulary': 'her', 'postings': [4]}, {'vocabulary': 'kan', 'postings': [4]}, {'vocabulary': 'du', 'postings': [4]}, {'vocabulary': 'studer', 'postings': [4]}, {'vocabulary': 'su', 'postings': [5]}, {'vocabulary': 'sp', 'postings': [6]}, {'vocabulary': 'stttemulighed', 'postings': [6]}, {'vocabulary': 'forskn', 'postings': [7]}, {'vocabulary': 'forskningsnyt', 'postings': [8]}, {'vocabulary': 'fra', 'postings': [8]}, {'vocabulary': 'ph.d.uddannels', 'postings': [9]}]\n",
      "query: aau AND aalborg OR viden AND verden OR og AND sidefag\n",
      "---\n",
      "['aau', 'AND', 'aalborg', 'OR', 'viden', 'AND', 'verden', 'OR', 'og', 'AND', 'sidefag']\n",
      "[[0, 1], 'OR', 'viden', 'AND', 'verden', 'OR', 'og', 'AND', 'sidefag']\n",
      "[[0, 1], 'OR', [0], 'OR', 'og', 'AND', 'sidefag']\n",
      "[[0, 1], 'OR', [0], 'OR', [3]]\n",
      "[[0, 1], 'OR', [3]]\n",
      "[[3, 0, 1]]\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "corpus = [] #collection of documents\n",
    "postings = []\n",
    "\n",
    "numOfArticles = 10\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")\n",
    "\n",
    "#construct the vocabulary of terms for your corpus (corpus)\n",
    "#build an 'inverted' index for your vocabulary (postings)\n",
    "\n",
    "# web crawls on the given link\n",
    "def getTitles(link):\n",
    "    titles = []\n",
    "\n",
    "    rp=RobotFileParser()\n",
    "    rp.set_url(link)\n",
    "    rp.read()\n",
    "    r=requests.get(link)\n",
    "\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    r_parse.find('title').string\n",
    "    for i, a in enumerate(r_parse.find_all('a')):\n",
    "        if(i == numOfArticles+1): break\n",
    "        _link = a['href']\n",
    "        if(_link == '#main'):   continue\n",
    "        if(_link[0] == '/'):    _link = link+_link[1:]\n",
    "        titles.append(_getTitles(_link))\n",
    "    return titles\n",
    "\n",
    "# takes the found links in the crawl and finds the title of them\n",
    "def _getTitles(link):\n",
    "    r=requests.get(link)\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    return r_parse.find('title').string\n",
    "\n",
    "def remove_non_ascii(a_str):\n",
    "    ascii_chars = set(string.printable)\n",
    "\n",
    "    return ''.join(\n",
    "        filter(lambda x: x in ascii_chars, a_str)\n",
    "    )\n",
    "\n",
    "def tokenizeAndStemTitles(titles):\n",
    "    _invertedIndex = [] # [{'vocabulary': 'aau', 'postings': [0, 1, 4]},...]\n",
    "    tokens = []\n",
    "\n",
    "    for i, title in enumerate(titles):\n",
    "        #first tokenize the document\n",
    "        _tokens=nltk.word_tokenize(title)\n",
    "\n",
    "        #then stem the tokens and remove unwanted characters.\n",
    "        ps=nltk.PorterStemmer()\n",
    "        for t in _tokens:\n",
    "            s = ps.stem(t)\n",
    "            s = remove_non_ascii(s)\n",
    "            s = s.replace(\"-\", \"\")\n",
    "            if(s == ''): continue\n",
    "            if s not in tokens: tokens.append(s)\n",
    "\n",
    "            flag = 0\n",
    "            for el in _invertedIndex:\n",
    "                if el['vocabulary'] == s:\n",
    "                    el['postings'].append(i) #add index to posting\n",
    "                    flag = 1\n",
    "\n",
    "            #if flag is 0, the only occurrence of the word is in the current document\n",
    "            if flag == 0: _invertedIndex.append(dict(vocabulary=s, postings=[i]))\n",
    "\n",
    "    return tokens, _invertedIndex\n",
    "\n",
    "titles = getTitles('https://www.aau.dk/')\n",
    "\n",
    "corpus, invertedIndex = tokenizeAndStemTitles(titles)\n",
    "print(\"corpus:\", corpus)\n",
    "print(\"inverted matrix:\", invertedIndex)\n",
    "\n",
    "#implement Boolean search for your index (perhaps only for a limited set of Boolean queries)\n",
    "validQueryOperators = [\"AND\", \"OR\"]\n",
    "query = \"aau AND aalborg OR viden AND verden OR og AND sidefag\"\n",
    "\n",
    "def getPosting(invertedIndex, wd):\n",
    "    for dict in invertedIndex:\n",
    "        if(dict['vocabulary'] == wd):\n",
    "            return dict['postings']\n",
    "    return False\n",
    "\n",
    "def booleanSearch(query, validQueryOperators, corpus):\n",
    "    t = [x for x in query.split(\" \")]\n",
    "\n",
    "    for wd in t:\n",
    "        if wd not in corpus and wd not in validQueryOperators:\n",
    "            print(wd, \"was not in the corpus. Try again.\")\n",
    "            return False\n",
    "\n",
    "    print(t)\n",
    "\n",
    "    t = merge(mergeAND, t, \"AND\")\n",
    "    t = merge(mergeOR, t, \"OR\")\n",
    "    \n",
    "    return sorted(t[0]) #from [[1,0,3]] --> [1,0,3]\n",
    "\n",
    "def merge(merge, t, boolOperator):\n",
    "    end = False\n",
    "    while not end:\n",
    "        for i,wd in enumerate(t):\n",
    "            if(wd == boolOperator):\n",
    "                #take operator as it is, or get the posting from inverted index\n",
    "                prev = t[i-1] if isinstance(t[i-1], list) else getPosting(invertedIndex, t[i-1])\n",
    "                next = t[i+1] if isinstance(t[i+1], list) else getPosting(invertedIndex, t[i+1])\n",
    "                t[i] = merge([prev,next])\n",
    "                del t[i+1]\n",
    "                del t[i-1]\n",
    "                print(t)\n",
    "\n",
    "        if boolOperator not in t:\n",
    "            end = True\n",
    "    return t\n",
    "\n",
    "def mergeOR(postings):\n",
    "    lstOr = []\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    pst0 = postings[0]\n",
    "    pst1 = postings[1]\n",
    "\n",
    "    len0 = len(postings[0])\n",
    "    len1 = len(postings[1])\n",
    "\n",
    "    done=False\n",
    "\n",
    "    while(not done):\n",
    "        #break if max length for both is reached, otherwise append the rest of the other list\n",
    "        if(idx0 > len0-1 and idx1 > len1-1):\n",
    "            break\n",
    "        elif(idx0 > len0-1):\n",
    "            lstOr.append(pst1[idx1])\n",
    "            idx1 += 1\n",
    "            continue\n",
    "        elif(idx1 > len1-1):\n",
    "            lstOr.append(pst0[idx0])\n",
    "            idx0 += 1\n",
    "            continue\n",
    "\n",
    "        if(pst0[idx0] > pst1[idx1]):\n",
    "            lstOr.append(pst0[idx0])\n",
    "            idx0 += 1\n",
    "        elif(pst1[idx1] > pst0[idx0]):\n",
    "            lstOr.append(pst1[idx1])\n",
    "            idx1 += 1\n",
    "        elif(pst0[idx0] == pst1[idx1]):\n",
    "            lstOr.append(pst0[idx0])\n",
    "            idx0 += 1\n",
    "            idx1 += 1\n",
    "\n",
    "    return lstOr\n",
    "\n",
    "def mergeAND(postings):\n",
    "    lstAnd = []\n",
    "    idx0 = 0\n",
    "    idx1 = 0\n",
    "    pst0 = postings[0]\n",
    "    pst1 = postings[1]\n",
    "\n",
    "    len0 = len(postings[0])\n",
    "    len1 = len(postings[1])\n",
    "\n",
    "    smallest = len1 if len1 < len0 else len0\n",
    "    \n",
    "    done=False\n",
    "    while(not done):\n",
    "        if(idx0 > smallest-1 or idx1 > smallest-1):\n",
    "            done = True\n",
    "        elif(pst0[idx0] > pst1[idx1]):\n",
    "            idx0 += 1\n",
    "        elif(pst1[idx1] > pst0[idx0]):\n",
    "            idx1 += 1\n",
    "        elif(pst0[idx0] == pst1[idx1]):\n",
    "            lstAnd.append(pst0[idx0])\n",
    "            idx0 += 1\n",
    "            idx1 += 1\n",
    "    return lstAnd\n",
    "\n",
    "print(\"query:\", query)\n",
    "print(\"---\")\n",
    "booleanSearch(query, validQueryOperators, corpus)\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is most useful for you depends on which websites you crawl. It is not essential for the exercise that the stemming always is the best possible ...!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
