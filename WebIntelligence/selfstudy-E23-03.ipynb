{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self Study 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This self study concludes our first \"miniproject\" on crawling and search. The tasks for this self study are:\n",
    "- modify/extend the inverted index you constructed in the previous self study to contain for all postings the term frequencies (if your documents are just the titles of the web pages, you will see very few term frequencies larger than 1, but do not worry about that).\n",
    "- calculate the idf values for all terms, and also include them in your index (cf. slide 3.20 for a schematic view)\n",
    "- implement ranked retrieval as described on slides 3.19 and 3.20 for the ntc.bnc similarity metric "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\minhs\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.robotparser import RobotFileParser\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus: ['aau', 'viden', 'for', 'verden', 'aalborg', 'universitet', 'universitetsuddannels', 'videregend', 'uddannels', 'p', 'kandidatuddannels', 'sidefag', 'og', 'tilvalgsfag', 'studieby', 'her', 'kan', 'du', 'studer', 'su', 'sp', 'stttemulighed', 'forskn', 'forskningsnyt', 'fra', 'ph.d.uddannels']\n",
      "inverted matrix: [{'vocabulary': 'aau', 'postings': [0, 1, 4]}, {'vocabulary': 'viden', 'postings': [0]}, {'vocabulary': 'for', 'postings': [0]}, {'vocabulary': 'verden', 'postings': [0]}, {'vocabulary': 'aalborg', 'postings': [0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]}, {'vocabulary': 'universitet', 'postings': [0, 1, 2, 2, 3, 3, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]}, {'vocabulary': 'universitetsuddannels', 'postings': [1]}, {'vocabulary': 'videregend', 'postings': [1]}, {'vocabulary': 'uddannels', 'postings': [1]}, {'vocabulary': 'p', 'postings': [1, 2, 3, 4, 5, 6, 7, 9]}, {'vocabulary': 'kandidatuddannels', 'postings': [2]}, {'vocabulary': 'sidefag', 'postings': [3]}, {'vocabulary': 'og', 'postings': [3]}, {'vocabulary': 'tilvalgsfag', 'postings': [3]}, {'vocabulary': 'studieby', 'postings': [4]}, {'vocabulary': 'her', 'postings': [4]}, {'vocabulary': 'kan', 'postings': [4]}, {'vocabulary': 'du', 'postings': [4]}, {'vocabulary': 'studer', 'postings': [4]}, {'vocabulary': 'su', 'postings': [5]}, {'vocabulary': 'sp', 'postings': [6]}, {'vocabulary': 'stttemulighed', 'postings': [6]}, {'vocabulary': 'forskn', 'postings': [7]}, {'vocabulary': 'forskningsnyt', 'postings': [8]}, {'vocabulary': 'fra', 'postings': [8]}, {'vocabulary': 'ph.d.uddannels', 'postings': [9]}]\n",
      "\n",
      "['term', 'df_t', 'idf_t']\n",
      "['aau', 3, 0.5228787452803376]\n",
      "['viden', 1, 1.0]\n",
      "['for', 1, 1.0]\n",
      "['verden', 1, 1.0]\n",
      "['aalborg', 10, 0.0]\n",
      "['universitet', 10, 0.0]\n",
      "['universitetsuddannels', 1, 1.0]\n",
      "['videregend', 1, 1.0]\n",
      "['uddannels', 1, 1.0]\n",
      "['p', 8, 0.09691001300805642]\n",
      "['kandidatuddannels', 1, 1.0]\n",
      "['sidefag', 1, 1.0]\n",
      "['og', 1, 1.0]\n",
      "['tilvalgsfag', 1, 1.0]\n",
      "['studieby', 1, 1.0]\n",
      "['her', 1, 1.0]\n",
      "['kan', 1, 1.0]\n",
      "['du', 1, 1.0]\n",
      "['studer', 1, 1.0]\n",
      "['su', 1, 1.0]\n",
      "['sp', 1, 1.0]\n",
      "['stttemulighed', 1, 1.0]\n",
      "['forskn', 1, 1.0]\n",
      "['forskningsnyt', 1, 1.0]\n",
      "['fra', 1, 1.0]\n",
      "['ph.d.uddannels', 1, 1.0]\n"
     ]
    }
   ],
   "source": [
    "titles = []\n",
    "corpus = []\n",
    "urls = []\n",
    "\n",
    "numOfArticles = 10\n",
    "\n",
    "dstemmer=SnowballStemmer(\"danish\")\n",
    "\n",
    "def getTitlesAndUrls(link):\n",
    "    titles = []\n",
    "\n",
    "    rp=RobotFileParser()\n",
    "    rp.set_url(link)\n",
    "    rp.read()\n",
    "    r=requests.get(link)\n",
    "\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    r_parse.find('title').string\n",
    "    for i, a in enumerate(r_parse.find_all('a')):\n",
    "        if(i == numOfArticles+1): break\n",
    "        _link = a['href']\n",
    "        if(_link == '#main'):   continue\n",
    "        if(_link[0] == '/'):    _link = link+_link[1:]\n",
    "        titles.append(_getTitles(_link))\n",
    "        urls.append(_link)\n",
    "    return titles, urls\n",
    "\n",
    "def _getTitles(link):\n",
    "    r=requests.get(link)\n",
    "    r_parse = BeautifulSoup(r.text, 'html.parser')\n",
    "    return r_parse.find('title').string\n",
    "\n",
    "def remove_non_ascii(a_str):\n",
    "    ascii_chars = set(string.printable)\n",
    "\n",
    "    return ''.join(\n",
    "        filter(lambda x: x in ascii_chars, a_str)\n",
    "    )\n",
    "\n",
    "def tokenizeAndStemTitles(titles):\n",
    "    _invertedIndex = []\n",
    "    tokens = []\n",
    "    for i, title in enumerate(titles):\n",
    "        _tokens=nltk.word_tokenize(title)\n",
    "        ps=nltk.PorterStemmer()\n",
    "        for t in _tokens:\n",
    "            s = ps.stem(t)\n",
    "            s = remove_non_ascii(s)\n",
    "            s = s.replace(\"-\", \"\")\n",
    "            if(s == ''): continue\n",
    "            if s not in tokens: tokens.append(s)\n",
    "\n",
    "            flag = 0\n",
    "            for el in _invertedIndex:\n",
    "                if el['vocabulary'] == s:\n",
    "                    el['postings'].append(i)\n",
    "                    flag = 1\n",
    "\n",
    "            if flag == 0: _invertedIndex.append(dict(vocabulary=s, postings=[i]))\n",
    "\n",
    "    return tokens, _invertedIndex\n",
    "\n",
    "titles, urls = getTitlesAndUrls('https://www.aau.dk/')\n",
    "\n",
    "corpus, invertedIndex = tokenizeAndStemTitles(titles)\n",
    "print(\"corpus:\", corpus)\n",
    "print(\"inverted matrix:\", invertedIndex)\n",
    "print()\n",
    "\n",
    "##new exercise:\n",
    "\n",
    "#1.modify/extend the inverted index you constructed in the previous self study to contain for all postings the term frequencies \n",
    "#(if your documents are just the titles of the web pages, you will see very few term frequencies larger than 1, but do not worry about that).\n",
    "\n",
    "def showTF(termFrequencyMatrix):\n",
    "    print(\"termFrequencyMatrix:\")\n",
    "    for row in termFrequencyMatrix:\n",
    "        print(row)\n",
    "\n",
    "termFrequencyMatrix = [[0 for _ in range(len(corpus)+1)] for _ in range(numOfArticles+1)]\n",
    "termFrequencyMatrix[0][0] = \"X\"\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    termFrequencyMatrix[0][i+1] = corpus[i]\n",
    "    for n in invertedIndex[i]['postings']:\n",
    "        termFrequencyMatrix[n+1][i+1] = invertedIndex[i]['postings'].count(n)\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    termFrequencyMatrix[i+1][0] = urls[i]\n",
    "\n",
    "#showTF(termFrequencyMatrix)\n",
    "\n",
    "#2.calculate the idf values for all terms, and also include them in your index (cf. slide 3.20 for a schematic view)\n",
    "# df(t) = sum(document in Corpus I[d,t])\n",
    "# idf(t) = log(N/df(t))\n",
    "\n",
    "#firstly make the term-document incident matrix for calculating df\n",
    "def column(matrix, i):\n",
    "    return [row[i] for row in matrix]\n",
    "\n",
    "def showIncidentMatrix(incidentMatrix):\n",
    "    for row in incidentMatrix:\n",
    "        print(row)\n",
    "\n",
    "def showidfTable(idfTable):\n",
    "    for row in idfTable:\n",
    "        print(row)\n",
    "\n",
    "incidentMatrix = [[0 for _ in range(len(corpus)+1)] for _ in range(numOfArticles+1)]\n",
    "incidentMatrix[0][0] = \"X\"\n",
    "for i in range(len(corpus)):\n",
    "    incidentMatrix[0][i+1] = corpus[i]\n",
    "    for n in invertedIndex[i]['postings']:\n",
    "        incidentMatrix[n+1][i+1] = 1\n",
    "\n",
    "for i in range(len(urls)):\n",
    "    incidentMatrix[i+1][0] = urls[i]\n",
    "\n",
    "idfTable = [[0 for _ in range(3)] for _ in range(len(corpus)+1)]\n",
    "idfTable[0] = ['term', 'df_t', 'idf_t']\n",
    "for i in range(len(corpus)):\n",
    "    _df = sum(column(incidentMatrix[1:], i+1))\n",
    "    idfTable[i+1][0] = incidentMatrix[0][i+1]\n",
    "    idfTable[i+1][1] = _df\n",
    "    idfTable[i+1][2] = math.log10(numOfArticles/_df)\n",
    "\n",
    "#showIncidentMatrix(incidentMatrix)\n",
    "showidfTable(idfTable)\n",
    "\n",
    "#3. implement ranked retrieval as described on slides 3.19 and 3.20 for the ntc.bnc similarity metric\n",
    "# F[d,t] * idf(t) * 1 / ||V~(d)|| "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 0, 0, 1, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "column(incidentMatrix[1:], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
